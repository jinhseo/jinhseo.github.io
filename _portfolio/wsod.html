<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <title>Latent Composition</title>

    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="jquery.js"></script>
    <style>
        body {
            font-family: 'Open-Sans', sans-serif;
            font-weight: 300;
            background-color: #fff;
        }

        .content {
            width: 1000px;
            padding: 25px 50px;
            margin: 25px auto;
            background-color: white;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }

        .contentblock {
            width: 950px;
            margin: 0 auto;
            padding: 0;
            border-spacing: 25px 0;
        }

        .contentblock td {
            background-color: #fff;
            padding: 25px 50px;
            vertical-align: top;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }

        a,
        a:visited {
            color: #224b8d;
            font-weight: 300;
        }

        #authors {
            text-align: center;
            margin-bottom: 20px;
        }

        #conference {
            text-align: center;
            margin-bottom: 20px;
            font-style: italic;
        }

        #authors a {
            margin: 0 10px;
        }

        h1 {
            text-align: center;
            font-size: 35px;
            font-weight: 300;
        }

        h2 {
            font-size: 30px;
            font-weight: 300;
        }

        code {
            display: block;
            padding: 10px;
            margin: 10px 10px;
        }

        p {
            line-height: 25px;
            text-align: justify;
        }

        p code {
            display: inline;
            padding: 0;
            margin: 0;
        }

        #teasers {
            margin: 0 auto;
        }

        #teasers td {
            margin: 0 auto;
            text-align: center;
            padding: 5px;
        }

        #teasers img {
            width: 250px;
        }

        #results img {
            width: 133px;
        }

        #seeintodark {
            margin: 0 auto;
        }

        #sift {
            margin: 0 auto;
        }

        #sift img {
            width: 250px;
        }

        .downloadpaper {
            padding-left: 20px;
            float: right;
            text-align: center;
        }

        .downloadpaper a {
            font-weight: bold;
            text-align: center;
        }

        #demoframe {
            border: 0;
            padding: 0;
            margin: 0;
            width: 100%;
            height: 340px;
        }

        #feedbackform {
            border: 1px solid #ccc;
            margin: 0 auto;
            border-radius: 15px;
        }

        #eyeglass {
            height: 530px;
        }

        #eyeglass #wrapper {
            position: relative;
            height: auto;
            margin: 0 auto;
            float: left;
            width: 800px;
        }

        #mitnews {
            font-weight: normal;
            margin-top: 20px;
            font-size: 14px;
            width: 220px;
        }

        #mitnews a {
            font-weight: normal;
        }

        .teaser-img {
            width: 80%;
						display: block;
						margin-left: auto;
						margin-right: auto;
        }
        .teaser-gif {
						display: block;
						margin-left: auto;
						margin-right: auto;
        }
        .summary-img {
            width: 100%;
						display: block;
						margin-left: auto;
						margin-right: auto;
        }

        .iframe {
            width: 100%;
            height: 125%
        }

      .container {
        display: flex;
        align-items: center;
        justify-content: center
      }
      .image {
        flex-basis: 40%
      }
      .text {
        font-size: 20px;
        padding-left: 20px;
      }
			.center {
				margin-left: auto;
				margin-right: auto;
			}
			.boxshadow {
				border: 1px solid;
				padding: 10px;
				box-shadow: 2px 2px 5px #888888;
			}
			.spacertr{
				height: 8px;
			}
			.spacertd{
				width: 40px;
			}

    </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
		<!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98008272-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-98008272-2');
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
		-->

</head>

<body>

    <div class="content">
			<h1>Using latent space regression to analyze and leverage compositionality in GANs</h1>
        <p id="authors">
            <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a>
						<a href="http://people.csail.mit.edu/jwulff/">Jonas Wulff</a>
            <a href="http://web.mit.edu/phillipi/">Phillip Isola</a><br>
            <!-- <strong>MIT Computer Science and Artificial Intelligence Laboratory</strong> -->
            MIT Computer Science and Artificial Intelligence Laboratory
						<br><i>International Conference on Learning Representations 2021</i>
        </p>
				<font size="+2">
					<p style="text-align: center;">
						<a href="http://arxiv.org/abs/2103.10426" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://github.com/chail/latent-composition" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="http://latent-composition.csail.mit.edu/presentations/latent_composition_poster.pdf" target="_blank">[Poster]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing" target="_blank">[Colab]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="bibtex.txt" target="_blank">[Bibtex]</a>
						<!--
						<a href="TODO: youtube link?" target="_blank">[Video]</a>
						-->
					</p>
				</font>
				<font size="+1">
					<p style="text-align: center;">
						Skip to:  &nbsp;&nbsp;
						<a href="#abstract">[Abstract]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="#summary">[Summary]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="#samples">[Uncurated Samples]</a> &nbsp;&nbsp;&nbsp;&nbsp;
					</p>
				</font>
        <p>
            <img class='teaser-img' src='img/teaser.jpeg'></img>
        </p>

				<p id="abstract"><strong>Abstract: </strong>
				In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs. We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and because it uses only a single forward pass, it can operate in real-time. 
				</p>

        <br clear="all">
    </div>
    <div class="content" id="summary">

        <h2 style="text-align:center;">Summary</h2>

				<div class="container">
					<div class="image">
						<img src='img/teaser.gif'></img>
					</div>
					<div class="text">
						<p> We use a latent regressor network that learns from missing data for image composition and image completion.  The combination of the regressor network and a pretrained GAN forms an image prior to create realistic images despite unrealistic input. This animation briefly demonstrates some applications of our method.</p>
					</div>
				</div>
        <br>
        <hr>
        <p style="text-align: center;">Using the latent regression and pretrained GAN, we can create automatic collages and merge them into coherent composite images.</p>
        <img class='summary-img' src='img/website_composition.jpeg'></img>

        <br>
        <hr>
        <p style="text-align: center;">We demonstrate an application of editing and merging real images. <br>To better fit a specific image, we do a few seconds of initial optimization, and the remaining editing occurs in real-time.</p>
        <img class='summary-img' style="width:80%;" src='img/finetune_edit.jpeg'></img>
        <br>
        <hr>
        <p style="text-align: center;">We use the latent regressor to investigate the independently changeable parts that the GAN learns from data. For example, we visualize what regions of a given image change, when the outlined red portion is modified. The resulting variations show regions of the images that commonly vary together, which can be interpreted as a form of unsupervised part discovery.</p>
        <img class='summary-img' src='img/website_independence.jpeg'></img>
			</div>
			<div class="content" id="samples">
        <h2 style="text-align:center;">Additional Samples</h2>
				<p style="text-align: center;">Click on the panels below to view uncurated composite images generated from randomly sampled image parts.</p>
				<table style="text-align: center;" class="center">
					<tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/proggan_church/">
								<p style="text-align: center;">ProGAN Church</p>
								<img src="img/samples_proggan_church_sample000002_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/proggan_livingroom/">
								<p style="text-align: center;">ProGAN Living Room</p>
								<img src="img/samples_proggan_livingroom_sample000000_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/proggan_celebahq/">
								<p style="text-align: center;">ProGAN CelebAHQ</p>
								<img src="img/samples_proggan_celebahq_sample000008_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
					</tr>
					<tr class="spacertr"></tr>
					<tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/stylegan_church/">
								<p style="text-align: center;">StyleGAN Church</p>
								<img src="img/samples_sgan_church_sample000000_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/stylegan_car/">
								<p style="text-align: center;">StyleGAN Car</p>
								<img src="img/samples_sgan_car_sample000006_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/stylegan_ffhq/">
								<p style="text-align: center;">StyleGAN FFHQ</p>
								<img src="img/samples_sgan_ffhq_sample000011_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
					</tr>
				</table>
				<br>
        <hr>
				<p style="text-align: center;">Click on the panels below to view comparisons of different image reconstruction methods operating on the same randomly sampled real image parts. Composition is a balance between unifying the input parts to create a realistic output, but still remaining close to the input parts; the methods exhibit a tradeoff of these two factors. A third axis is inference time. In the below webpages, methods that require additional per-image optimization are marked with (*), whereas the other methods operate using a single forward pass. </p>
				<table style="text-align: center;" class="center">
					<tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/comparisons/celebahq_face/">
								<p style="text-align: center;">CelebAHQ Faces</p>
								<img src="img/samples_celebahq_face_input.png" style="width:150px">
							</a>
							</div>
						</td>
						<td class="spacertd">
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/comparisons/lsun_church/">
								<p style="text-align: center;">LSUN Church</p>
								<img src="img/samples_lsun_church_input.png" style="width:150px">
							</a>
							</div>
						</td>
					</tr>
				</table>
    </div>      
    <div class="content" id="references">

        <h2>Reference</h2>

				<p>L Chai, J Wulff, P Isola. Using latent space regression to analyze and leverage compositionality in GANs. <br>International Conference on Learning Representations, 2021.</p>

        <code>
			@inproceedings{chai2021latent,<br>
				&nbsp;&nbsp;title={Using latent space regression to analyze and leverage compositionality in GANs.},<br>
				&nbsp;&nbsp;author={Chai, Lucy and Wulff, Jonas and Isola, Phillip},<br>
				&nbsp;&nbsp;booktitle={International Conference on Learning Representations},<br>
				&nbsp;&nbsp;year={2021}<br>
			 }
				</code>
    </div>      
    <div class="content" id="acknowledgements">
          <p><strong>Acknowledgements</strong>:
					We would like to thank David Bau, Richard Zhang, Tongzhou Wang, Luke Anderson, and Yen-Chen Lin for helpful discussions and feedback. Thanks to Antonio Torralba, Alyosha Efros, Richard Zhang, Jun-Yan Zhu, Wei-Chiu Ma, Minyoung Huh, and Yen-Chen Lin for permission to use their photographs. LC is supported by the National Science Foundation Graduate Research Fellowship under Grant No. 1745302. JW is supported by a grant from Intel Corp.
					 Recycling a familiar <a href="https://chail.github.io/patch-forensics/">template</a> ;).
    </div>
</body>

</html>
